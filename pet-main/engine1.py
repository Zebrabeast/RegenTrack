"""
Train and eval functions used in main.py
"""
import math
import os
import sys
from typing import Iterable
import numpy as np
import cv2

import torch
import torchvision.transforms as standard_transforms
import torch.nn.functional as F

import util.misc as utils
from util.misc import NestedTensor


class DeNormalize(object):
    def __init__(self, mean, std):
        self.mean = mean
        self.std = std

    def __call__(self, tensor):
        for t, m, s in zip(tensor, self.mean, self.std):
            t.mul_(s).add_(m)
        return tensor


def visualization(samples, targets, pred, vis_dir, split_map=None):
    """
    Visualize predictions
    """
    gts = [t['points'].tolist() for t in targets]

    pil_to_tensor = standard_transforms.ToTensor()

    restore_transform = standard_transforms.Compose([
        DeNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        standard_transforms.ToPILImage()
    ])

    images = samples.tensors
    masks = samples.mask
    for idx in range(images.shape[0]):
        sample = restore_transform(images[idx])
        sample = pil_to_tensor(sample.convert('RGB')).numpy() * 255
        sample_vis = sample.transpose([1, 2, 0])[:, :, ::-1].astype(np.uint8).copy()

        # draw ground-truth points (red)
        size = 2
        for t in gts[idx]:
            sample_vis = cv2.circle(sample_vis, (int(t[1]), int(t[0])), size, (0, 0, 255), -1)

        # draw predictions (green)
        for p in pred[idx]:
            sample_vis = cv2.circle(sample_vis, (int(p[1]), int(p[0])), size, (0, 255, 0), -1)

        # draw split map
        if split_map is not None:
            imgH, imgW = sample_vis.shape[:2]
            split_map = (split_map * 255).astype(np.uint8)
            split_map = cv2.applyColorMap(split_map, cv2.COLORMAP_JET)
            split_map = cv2.resize(split_map, (imgW, imgH), interpolation=cv2.INTER_NEAREST)
            sample_vis = split_map * 0.9 + sample_vis

        # save image
        if vis_dir is not None:
            # eliminate invalid area
            imgH, imgW = masks.shape[-2:]
            valid_area = torch.where(~masks[idx])
            valid_h, valid_w = valid_area[0][-1], valid_area[1][-1]
            sample_vis = sample_vis[:valid_h + 1, :valid_w + 1]

            name = targets[idx]['image_path'].split('/')[-1].split('.')[0]
            cv2.imwrite(os.path.join(vis_dir, '{}_gt{}_pred{}.jpg'.format(name, len(gts[idx]), len(pred[idx]))),
                        sample_vis)


# Create another visualization function to modify the output.
def visualization1(samples, targets, pred, vis_dir, split_map=None):
    """
    Visualize predictions
    """
    gts = [t['points'].tolist() for t in targets]

    pil_to_tensor = standard_transforms.ToTensor()

    restore_transform = standard_transforms.Compose([
        DeNormalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        standard_transforms.ToPILImage()
    ])

    images = samples.tensors
    masks = samples.mask
    for idx in range(images.shape[0]):
        sample = restore_transform(images[idx])
        sample = pil_to_tensor(sample.convert('RGB')).numpy() * 255
        sample_vis = sample.transpose([1, 2, 0])[:, :, ::-1].astype(np.uint8).copy()

        # draw ground-truth points (red)
        size = 2
        for t in gts[idx]:
            # sample_vis = cv2.circle(sample_vis, (int(t[1]), int(t[0])), size, (0, 0, 255), -1)
            # 2024.11.21改：
            # if len(t) >= 2:  # Ensure there are at least two elements for indexing
            #     sample_vis = cv2.circle(sample_vis, (int(t[1]), int(t[0])), size, (0, 0, 255), -1)
            # 还是报错，2024.11.22改：
            if isinstance(t, torch.Tensor) and t.dim() >= 1 and len(t) >= 2:
                sample_vis = cv2.circle(sample_vis, (int(t[1]), int(t[0])), size, (0, 0, 255), -1)

        # record [p_id,x,y] for save txt file
        person_id = 0
        pred_positions = []  # Store prediction positions for saving to txt file
        # draw predictions (green)
        for p in pred[idx]:
            # sample_vis = cv2.circle(sample_vis, (int(p[1]), int(p[0])), size, (0, 255, 0), -1)
            # pred_positions.append([person_id, int(p[1]), int(p[0])])
            # person_id +=1
            # 2024.11.21改：
            # if len(p) >= 2:  # Ensure there are at least two elements for indexing
            #     sample_vis = cv2.circle(sample_vis, (int(p[1]), int(p[0])), size, (0, 255, 0), -1)
            #     pred_positions.append([person_id, int(p[1]), int(p[0])])
            #     person_id += 1
            # 还是报错，2024.11.22改：
            if isinstance(p, torch.Tensor) and p.dim() >= 1 and len(p) >= 2:
                sample_vis = cv2.circle(sample_vis, (int(p[1]), int(p[0])), size, (0, 255, 0), -1)
                pred_positions.append([person_id, int(p[1]), int(p[0])])
                person_id += 1

        # draw split map
        if split_map is not None:
            imgH, imgW = sample_vis.shape[:2]
            split_map = (split_map * 255).astype(np.uint8)
            split_map = cv2.applyColorMap(split_map, cv2.COLORMAP_JET)
            split_map = cv2.resize(split_map, (imgW, imgH), interpolation=cv2.INTER_NEAREST)
            sample_vis = split_map * 0.9 + sample_vis

        # save image
        if vis_dir is not None:
            # Create two folders to facilitate partition management of the two outputs

            # save visualization image
            vis_img_root = os.path.join(vis_dir, 'vis_img')
            if not os.path.exists(vis_img_root):
                os.makedirs(vis_img_root)

            # save individual coordinate
            txt_root = os.path.join(vis_dir, 'txt')
            if not os.path.exists(txt_root):
                os.makedirs(txt_root)

            # eliminate invalid area
            imgH, imgW = masks.shape[-2:]
            valid_area = torch.where(~masks[idx])
            valid_h, valid_w = valid_area[0][-1], valid_area[1][-1]
            sample_vis = sample_vis[:valid_h + 1, :valid_w + 1]

            name = targets[idx]['image_path'].split('/')[-1].split('.')[0]
            cv2.imwrite(os.path.join(vis_img_root, '{}_gt{}_pred{}.jpg'.format(name, len(gts[idx]), len(pred[idx]))),
                        sample_vis)

            # save predictions to txt file
            txt_path = os.path.join(txt_root, '{}_pred_positions.txt'.format(name))
            with open(txt_path, 'w') as f:
                for person in pred_positions:
                    f.write('{},{},{}'.format(person[0], person[1], person[2]))
                    f.write('\n')


# training
def train_one_epoch(model: torch.nn.Module, criterion: torch.nn.Module,
                    data_loader: Iterable, optimizer: torch.optim.Optimizer,
                    device: torch.device, epoch: int, max_norm: float = 0):
    model.train()
    criterion.train()
    metric_logger = utils.MetricLogger(delimiter="  ")
    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))
    header = 'Epoch: [{}]'.format(epoch)
    print_freq = 10

    for samples, targets in metric_logger.log_every(data_loader, print_freq, header):
        samples = samples.to(device)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
        gt_points = [target['points'] for target in targets]

        outputs = model(samples, epoch=epoch, train=True,
                        criterion=criterion, targets=targets)
        loss_dict, weight_dict, losses = outputs['loss_dict'], outputs['weight_dict'], outputs['losses']

        # reduce losses over all GPUs for logging purposes
        loss_dict_reduced = utils.reduce_dict(loss_dict)
        loss_dict_reduced_unscaled = {f'{k}_unscaled': v
                                      for k, v in loss_dict_reduced.items()}
        loss_dict_reduced_scaled = {k: v * weight_dict[k]
                                    for k, v in loss_dict_reduced.items() if k in weight_dict}
        losses_reduced_scaled = sum(loss_dict_reduced_scaled.values())

        loss_value = losses_reduced_scaled.item()

        if not math.isfinite(loss_value):
            print("Loss is {}, stopping training".format(loss_value))
            print(loss_dict_reduced)
            sys.exit(1)

        optimizer.zero_grad()
        losses.backward()
        if max_norm > 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)
        optimizer.step()

        metric_logger.update(loss=loss_value, **loss_dict_reduced_scaled, **loss_dict_reduced_unscaled)
        metric_logger.update(lr=optimizer.param_groups[0]["lr"])

    # gather the stats from all processes
    metric_logger.synchronize_between_processes()
    print("Averaged stats:", metric_logger)
    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}


# 2024.11.21改：希望可以使用nms改善多个坐标给一个人的情况：
# def nms(points, scores, dist_thresh):
#     # 确定当前设备（CPU或GPU）
#     device = points.device if isinstance(points, torch.Tensor) else torch.device("cuda" if torch.cuda.is_available() else "cpu")
#
#     # 确保点的坐标和得分是张量并移动到相同的设备
#     points = torch.tensor(points, dtype=torch.float32, device=device)
#     scores = torch.tensor(scores, dtype=torch.float32, device=device)
#
#     # 检查点的数量
#     if points.shape[0] == 0:
#         return torch.empty((0, 2), dtype=torch.float32, device=device)
#
#     # 按置信度排序点的索引
#     sorted_indices = torch.argsort(scores, descending=True).to(device)
#
#     # 初始化一个列表来保存最终保留下来的点的索引
#     keep_indices = []
#
#     while sorted_indices.numel() > 0:
#         # 选择当前置信度最高的点
#         current_index = sorted_indices[0]
#         keep_indices.append(current_index)
#
#         # 获取当前保留点的坐标
#         current_point = points[current_index]
#
#         # 计算与当前点之间的距离（只计算剩余点）
#         remaining_points = points[sorted_indices]
#         distances = torch.cdist(current_point.view(1, -1), remaining_points).squeeze(0)
#
#         # 找到距离大于阈值的点的索引
#         mask = distances > dist_thresh
#         sorted_indices = sorted_indices[mask]
#
#     # 返回保留下来的点的坐标
#     return points[keep_indices]
# 代码报错，2024.11.22修改：
def nms(points, scores, dist_thresh):
    # 确定当前设备（CPU或GPU）
    device = points.device if isinstance(points, torch.Tensor) else torch.device(
        "cuda" if torch.cuda.is_available() else "cpu")

    # 确保点的坐标和得分是张量并移动到相同的设备
    points = torch.tensor(points, dtype=torch.float32, device=device)
    scores = torch.tensor(scores, dtype=torch.float32, device=device)

    if scores.size(0) == 1:
        return points
    # 检查点的数量
    if points.shape[0] == 0:
        return torch.empty((0, 2), dtype=torch.float32, device=device)

    # 按置信度排序点的索引
    sorted_indices = torch.argsort(scores, descending=True).to(device)

    # 初始化一个列表来保存最终保留下来的点的索引
    keep_indices = []

    while sorted_indices.numel() > 0:
        # 选择当前置信度最高的点
        current_index = sorted_indices[0]
        keep_indices.append(current_index)

        # 获取当前保留点的坐标
        current_point = points[current_index]

        # 获取剩余点并计算距离
        remaining_points = points[sorted_indices]
        if remaining_points.shape[0] > 1:
            distances = torch.cdist(current_point.view(1, -1), remaining_points).squeeze(0)
        else:
            distances = torch.tensor([0.0], device=device)

        # 找到距离大于阈值的点的索引
        mask = distances > dist_thresh
        sorted_indices = sorted_indices[mask]

    # 返回保留下来的点的坐标
    return points[torch.tensor(keep_indices, device=device, dtype=torch.long)]


# evaluation
@torch.no_grad()
def evaluate(model, data_loader, device, epoch=0, vis_dir=None):
    model.eval()

    metric_logger = utils.MetricLogger(delimiter="  ")
    header = 'Test:'

    if vis_dir is not None:
        os.makedirs(vis_dir, exist_ok=True)

    print_freq = 10
    for samples, targets in metric_logger.log_every(data_loader, print_freq, header):
        samples = samples.to(device)
        img_h, img_w = samples.tensors.shape[-2:]

        # inference
        outputs = model(samples, test=True, targets=targets)
        outputs_scores = torch.nn.functional.softmax(outputs['pred_logits'], -1)[:, :, 1][0]
        outputs_points = outputs['pred_points'][0]
        outputs_offsets = outputs['pred_offsets'][0]

        # process predicted points
        predict_cnt = len(outputs_scores)
        print(predict_cnt)
        print(len(outputs_points))
        gt_cnt = targets[0]['points'].shape[0]

        # compute error
        mae = abs(predict_cnt - gt_cnt)
        mse = (predict_cnt - gt_cnt) * (predict_cnt - gt_cnt)

        # record results
        results = {}
        toTensor = lambda x: torch.tensor(x).float().cuda()
        results['mae'], results['mse'] = toTensor(mae), toTensor(mse)
        metric_logger.update(mae=results['mae'], mse=results['mse'])

        results_reduced = utils.reduce_dict(results)
        metric_logger.update(mae=results_reduced['mae'], mse=results_reduced['mse'])

        # visualize predictions
        if vis_dir:
            points = [[point[0] * img_h, point[1] * img_w] for point in outputs_points]  # recover to actual points
            # 2024.11.21改：
            dist_thresh = 20.0
            kept_points = nms(points, outputs_scores, dist_thresh)
            split_map = (outputs['split_map_raw'][0].detach().cpu().squeeze(0) > 0.5).float().numpy()
            # visualization1(samples, targets, [points], vis_dir, split_map=split_map)
            # 2024.11.21改：
            visualization1(samples, targets, [kept_points], vis_dir, split_map=split_map)

    # gather the stats from all processes
    metric_logger.synchronize_between_processes()
    results = {k: meter.global_avg for k, meter in metric_logger.meters.items()}
    results['mse'] = np.sqrt(results['mse'])
    return results
